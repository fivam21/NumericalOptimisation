{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.linalg import orth\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import root_scalar\n",
    "import re\n",
    "\n",
    "class Functions:\n",
    "    \n",
    "    def __init__(self, f, df, d2f):\n",
    "            self.f = f\n",
    "            self.df = df\n",
    "            self.d2f = d2f\n",
    "\n",
    "    def zoomInt(self, phi, dphi, alpha_l, alpha_h, c1, c2, trail_step = 'bisection'):\n",
    "        ''' The purpose of this algorithm is to find a suitable step length (alpha) \n",
    "        that satisfies the Wolfe conditions during a line search in an optimization problem.\n",
    "        Successively decrease the inverval size of the alphas until acceptable step length found'''\n",
    "\n",
    "        tol = np.finfo(float).eps         \n",
    "        # Structure containing information about the iteration\n",
    "        info = {'alpha_ls': [], 'alpha_hs': [], 'alpha_js': [], 'phi_js': [], 'dphi_js': []}\n",
    "\n",
    "        n = 1\n",
    "        stop = False\n",
    "        max_iter = 100\n",
    "        alpha = 0\n",
    "\n",
    "        while n < max_iter and not stop:\n",
    "            # Find trial step length alpha_j in [alpha_l, alpha_h]\n",
    "            if trail_step.lower() == 'bisection':\n",
    "                alpha_j = 0.5 * (alpha_h + alpha_l)\n",
    "            elif trail_step.lower() == 'interp2':\n",
    "                # You need to implement the 'interp2' method\n",
    "                raise NotImplementedError(\"Interp2 method not implemented\")\n",
    "\n",
    "            phi_j = phi(alpha_j)\n",
    "\n",
    "            # Update info\n",
    "            info['alpha_ls'].append(alpha_l)\n",
    "            info['alpha_hs'].append(alpha_h)\n",
    "            info['alpha_js'].append(alpha_j)\n",
    "            info['phi_js'].append(phi_j)\n",
    "\n",
    "            if abs(alpha_h - alpha_l) < tol:\n",
    "                alpha = alpha_j\n",
    "                stop = True\n",
    "                print(\"Line search stopped because the interval became too small. Returning center of the interval.\")\n",
    "                print(f'Centre: {alpha_h - alpha_l}')\n",
    "                break\n",
    "\n",
    "            if phi_j > phi(0) + c1 * alpha_j * dphi(0) or phi(alpha_j) >= phi(alpha_l):\n",
    "                # alpha_j does not satisfy sufficient decrease condition look for alpha < alpha_j or phi(alpha_j) >= phi(alpha_l)\n",
    "                alpha_h = alpha_j\n",
    "                info['dphi_js'].append(np.nan)\n",
    "\n",
    "            else:\n",
    "                # alpha_j satisfies sufficient decrease condition\n",
    "                dphi_j = dphi(alpha_j)\n",
    "                info['dphi_js'].append(dphi_j)\n",
    "\n",
    "                if abs(dphi_j) <= -c2 * dphi(0):\n",
    "                    # alpha_j satisfies strong curvature condition\n",
    "                    alpha = alpha_j\n",
    "                    stop = True\n",
    "\n",
    "                elif dphi_j * (alpha_h - alpha_l) >= 0:\n",
    "                    # alpha_h : dphi(alpha_l)*(alpha_h - alpha_l) < 0\n",
    "                    # alpha_j violates this condition but swapping alpha_l <-> alpha_h will reestablish it\n",
    "                    # [alpha_j, alpha_l]\n",
    "                    alpha_h = alpha_l\n",
    "                \n",
    "                alpha_l = alpha_j\n",
    "\n",
    "            n += 1\n",
    "\n",
    "        return alpha, info\n",
    "    \n",
    "    def lineSearch(self, x_k, p_k, alpha_max, alpha0, opts={'c1': 1e-4, 'c2': 0.9}):\n",
    "\n",
    "        # Parameters\n",
    "        w = 0.9\n",
    "\n",
    "        ## Function phi(alpha) = f(x_k + alpha*p_k)\n",
    "        def phi(alpha):\n",
    "            return self.f(x_k + alpha * p_k)\n",
    "\n",
    "        ## Derivative dphi(alpha) = df(x_k + alpha*p_k)' * p_k\n",
    "        def dphi(alpha):\n",
    "            return self.df(x_k + alpha * p_k).T @ p_k\n",
    "\n",
    "        # Initialization\n",
    "        alpha = [alpha0, alpha_max] # alpha0 should be 0\n",
    "        phi_i = [phi(0)]\n",
    "        dphi_i = [dphi(0)]\n",
    "        alpha_s = 0\n",
    "        n = 1\n",
    "        max_iter = 10\n",
    "        stop = False\n",
    "\n",
    "        while n < max_iter and not stop:\n",
    "            phi_i.append(phi(alpha[n]))\n",
    "            dphi_i.append(dphi(alpha[n]))\n",
    "            # check if alpha(n) satisfies the Wolfe conditions\n",
    "            if (phi_i[n] > phi_i[0] + opts['c1'] * alpha[n] * dphi_i[0] or (phi_i[n] >= phi_i[n - 1] and n > 1)):\n",
    "                alpha_s, _ = Functions.zoomInt(self, phi, dphi, alpha_l = alpha[n - 1], alpha_h = alpha[n], c1=opts['c1'], c2=opts['c2'], trail_step = 'bisection')\n",
    "                stop = True\n",
    "            #evaluate phi'(alpha_i) - check strong curvature condition:\n",
    "            elif (abs(dphi_i[n]) <= opts['c2'] * dphi_i[0]):\n",
    "                alpha_s = alpha[n]\n",
    "                stop = True\n",
    "            elif (dphi_i[n] >= 0):\n",
    "                alpha_s, _ =  Functions.zoomInt(self, phi, dphi, alpha_l=alpha[n], alpha_h=alpha[n-1], c1=opts['c1'], c2=opts['c2'])\n",
    "                stop = True\n",
    "\n",
    "            # Choose alpha(n+1) in (alpha(n), alpha_max)\n",
    "            alpha.append(w * alpha[n] + (1 - w) * alpha_max)\n",
    "            n += 1\n",
    "\n",
    "            # Maximal number of iterations reached. Return the last alpha(n+1).\n",
    "            if n == max_iter:\n",
    "                alpha_s = alpha[n]\n",
    "        \n",
    "        return alpha_s, {'alphas': alpha}\n",
    "    \n",
    "            \n",
    "    def backtracking(self, x_k, p, alpha0, opts={'c1': 1e-4, 'rho': None}):\n",
    "\n",
    "        if opts['c1'] == None:\n",
    "            c1 = 1e-4\n",
    "        else:\n",
    "            c1 = opts['c1']\n",
    "        # rho = 0.1 for steepest descent, conjugate gradients\n",
    "        # rho = 0.9 for Newton, Quasi-Newton\n",
    "        if opts['rho'] == None:\n",
    "            rho = 0.2\n",
    "        else:\n",
    "            rho = opts['rho']\n",
    "        if alpha0 == None:\n",
    "            alpha0 = 1\n",
    "\n",
    "\n",
    "        # Initialize info structure\n",
    "        info = {'alphas': [alpha0], 'rho': [rho], 'c1': [c1]}\n",
    "\n",
    "        # Initial step length\n",
    "        alpha = alpha0\n",
    "\n",
    "        # Compute f, grad f at x_k\n",
    "        f_k = self.f(x_k)\n",
    "        df_k = self.df(x_k)\n",
    "\n",
    "        # Backtracking line search\n",
    "        while self.f(x_k + alpha * p) > f_k + c1 * alpha * np.dot(df_k, p):\n",
    "            alpha = rho * alpha\n",
    "            info['alphas'].append(alpha)\n",
    "\n",
    "        return alpha, info\n",
    "    \n",
    "\n",
    "    def back_stepsize(self, x_k, p_k, alpha0, c, p):\n",
    "        # Function phi(alpha) = f(x_k + alpha*p_k)\n",
    "        def phi(alpha):\n",
    "            return self.f(x_k + alpha * p_k)\n",
    "\n",
    "        # Derivative dphi(alpha) = df(x_k + alpha*p_k)' * p_k\n",
    "        def dphi(alpha):\n",
    "            return np.dot(self.df(x_k + alpha * p_k).T, p_k) # wrap in list??\n",
    "\n",
    "        alpha = [alpha0]\n",
    "        phi_i = [phi(0)]\n",
    "        dphi_i = [dphi(0)]\n",
    "        alpha_s = 0\n",
    "        n = 0\n",
    "        max_iter = 10\n",
    "        stop = False\n",
    "\n",
    "        while n < max_iter and not stop:\n",
    "\n",
    "            # check if alpha(n) satisfies the sufficient decrease condition\n",
    "            if phi(alpha[n]) <= (phi(0) + c * alpha[n] * dphi(alpha[n])):\n",
    "                alpha_s = alpha[n] * p\n",
    "                stop = True\n",
    "\n",
    "            # Choose alpha(n+1) in (alpha(n), alpha_max)\n",
    "            alpha.append(alpha[n])\n",
    "            phi_i.append(phi(alpha[n]))\n",
    "            dphi_i.append(dphi(alpha[n]))\n",
    "\n",
    "            n += 1\n",
    "\n",
    "            # Maximal number of iterations reached. Return the last alpha(n+1).\n",
    "            if n == max_iter:\n",
    "                print('Max iterations reached, setting alpha = alpha[n]*p')\n",
    "                alpha_s = alpha[n] * p\n",
    "        \n",
    "        return alpha_s, {'alphas': alpha}\n",
    "    \n",
    "\n",
    "    def descentLineSearch(self, x0, alpha0, alpha_max, tol, maxIter, ls ='linesearch', switch='steepest',\n",
    "                        stop_type = 'grad', fixed_step = False, fixed_alpha=0.1):\n",
    "\n",
    "        # Initialization\n",
    "        nIter = 0\n",
    "        x_k = x0\n",
    "        info = {'xs': [np.array(x0)], 'alphas': [alpha0]}\n",
    "        stopCond = False\n",
    "\n",
    "        if not fixed_step:\n",
    "            # Loop until convergence or maximum number of iterations\n",
    "            while not stopCond and nIter <= maxIter:\n",
    "\n",
    "                nIter += 1\n",
    "\n",
    "                # Compute descent direction\n",
    "                if switch.lower() == 'steepest':\n",
    "                    # p_k steepest descent direction\n",
    "                    p_k = -self.df(x_k)\n",
    "                elif switch.lower() == 'newton':                    \n",
    "                    p_k = -np.linalg.inv(self.d2f(x_k)).dot(self.df(x_k)) # solution to Ax = b is x = A\\b\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown descent method '{switch}'. Expected 'steepest' or 'newton'.\")\n",
    "\n",
    "                # Call line search to compute step length alpha_k\n",
    "                if ls.lower() =='linesearch':\n",
    "                    alpha_k, _ = Functions.lineSearch(x_k=x_k, alpha0=alpha0, p_k=p_k, \n",
    "                                alpha_max=alpha_max, opts = {'c1': 1e-4, 'c2': 0.9}) \n",
    "                    \n",
    "                elif ls.lower() == 'backtracking':\n",
    "                    alpha_k, _ = Functions.backtracking(x_k, p_k, alpha0, opts={'c1': 1e-4, 'rho': 0.2})\n",
    "                    \n",
    "                \n",
    "                # Update x_k\n",
    "                x_k_1 = x_k\n",
    "                x_k = x_k + alpha_k * p_k\n",
    "\n",
    "                # Store iteration info\n",
    "                info['xs'].append(x_k)\n",
    "                info['alphas'].append(alpha_k)\n",
    "\n",
    "                # Stopping conditions\n",
    "                if stop_type.lower() == 'step':\n",
    "                    # Compute relative step length\n",
    "                    normStep = norm(x_k - x_k_1) / norm(x_k_1)\n",
    "                    stopCond = (normStep < tol)\n",
    "                elif stop_type.lower() == 'grad':\n",
    "                    stopCond = (norm(self.df(x_k)) < tol * (1 + abs(self.f(x_k)))) # np.inf\n",
    "\n",
    "\n",
    "            # Assign output values\n",
    "            xMin = x_k\n",
    "            fMin = self.f(x_k)\n",
    "\n",
    "        else:\n",
    "            while not stopCond and nIter <= maxIter:\n",
    "\n",
    "                nIter += 1\n",
    "                # Compute descent direction\n",
    "                if switch.lower() == 'steepest':\n",
    "                    # p_k steepest descent direction\n",
    "                    p_k = -self.df(x_k)\n",
    "                elif switch.lower() == 'newton':\n",
    "                    # p_k Newton direction\n",
    "                    p_k = -np.linalg.inv(self.d2f(x_k)).dot(self.df(x_k))\n",
    "\n",
    "                alpha_k = fixed_alpha\n",
    "\n",
    "                # Update x_k\n",
    "                x_k_1 = x_k\n",
    "                x_k = x_k + alpha_k * p_k\n",
    "\n",
    "                # Store iteration info\n",
    "                info['xs'].append(x_k)\n",
    "                info['alphas'].append(alpha_k)\n",
    "\n",
    "                # Stopping conditions\n",
    "                if stop_type.lower() == 'step':\n",
    "                    # Compute relative step length\n",
    "                    normStep = norm(x_k - x_k_1) / norm(x_k_1)\n",
    "                    stopCond = (normStep < tol)\n",
    "                elif stop_type.lower() == 'grad':\n",
    "                    stopCond = (norm(self.df(x_k)) < tol * (1 + abs(self.f(x_k)))) # np.inf\n",
    "\n",
    "\n",
    "            # Assign output values\n",
    "            xMin = x_k\n",
    "            fMin = self.f(x_k)\n",
    "\n",
    "        return xMin, fMin, nIter, info\n",
    "    \n",
    "\n",
    "    def backtrack_LineSearch(self, x0, alpha0=1, tol=0.1, maxIter=1000, switch='steepest', \n",
    "                            stop_type = 'grad', c1=1, c2=0.1, p=0.2):\n",
    "\n",
    "        # Initialization\n",
    "        nIter = 0\n",
    "        x_k = x0\n",
    "        info = {'xs': [np.array([x0])], 'alphas': [alpha0]}\n",
    "        stopCond = False\n",
    "        opts = {}\n",
    "\n",
    "        # Loop until convergence or maximum number of iterations\n",
    "        while not stopCond and nIter <= maxIter:\n",
    "\n",
    "            nIter += 1\n",
    "\n",
    "            # Compute descent direction\n",
    "            if switch.lower() == 'steepest':\n",
    "                # p_k steepest descent direction\n",
    "                p_k = -1 * self.df(x_k)\n",
    "                alpha0 = 1\n",
    "            elif switch.lower() == 'newton':\n",
    "                # p_k = -1 * d2f(x_k)^-1 / df(x_k)\n",
    "                p_k = -np.linalg.inv(self.d2f(x_k)).dot(self.df(x_k))\n",
    "                # c2 = 1-c1\n",
    "                # solution to Ax = b is x = A\\b\n",
    "\n",
    "            # Call line search to compute step length alpha_k\n",
    "            alpha_k, _ = Functions.back_stepsize(x_k, p_k, alpha0, c=c1, p=p) # or backtracking(self, x_k, p, alpha0, opts={'c1': 1e-4, 'rho': 0.9})\n",
    "            # back_stepsize(self, x_k, p_k, alpha0, c, p)\n",
    "\n",
    "            # Update x_k\n",
    "            x_k_1 = x_k\n",
    "            x_k = x_k + alpha_k * p_k\n",
    "\n",
    "            # Store iteration info\n",
    "            info['xs'].append(x_k)\n",
    "            info['alphas'].append(alpha_k)\n",
    "\n",
    "            # Stopping conditions\n",
    "            if stop_type == 'step':\n",
    "                # Compute relative step length\n",
    "                normStep = norm(x_k - x_k_1) / norm(x_k_1)\n",
    "                stopCond = (normStep < tol)\n",
    "            elif stop_type == 'grad':\n",
    "                stopCond = (norm(self.df(x_k)) < tol * (1 + abs(self.f(x_k)))) # np.inf\n",
    "\n",
    "\n",
    "        # Assign output values\n",
    "        xMin = x_k\n",
    "        fMin = self.f(x_k)\n",
    "\n",
    "        return xMin, fMin, nIter, info\n",
    "    \n",
    "\n",
    "    def root_finder(t, p_U, a, trust_radius_Delta):\n",
    "        eqn = np.linalg.norm(p_U + (t-1)*(a - p_U))**2 - trust_radius_Delta**2\n",
    "\n",
    "        # Find the root using scipy.optimize.root_scalar\n",
    "        result = root_scalar(eqn, bracket=[0, 5], method='bisect', x0=1.5)\n",
    "        tau2 = result.root \n",
    "        return tau2\n",
    "   \n",
    "   \n",
    "    def trustRegion(self, x0, p, trust_radius_Delta, eta, \n",
    "                    tol, maxIter, stopType = 'grad'): \n",
    "        \n",
    "        # Initialisation\n",
    "        current_radius = 0.5 * trust_radius_Delta\n",
    "        stopCond = False\n",
    "        x_k = x0\n",
    "        nTaken = 0\n",
    "        k = 0\n",
    "\n",
    "        info = {'xs':[np.array(x_k)], 'xind':[], 'rhos':[], 'Deltas':[], 'stopCond':[]}\n",
    "        info['xind'] = np.zeros(maxIter+1)\n",
    "\n",
    "        while not stopCond and (k < maxIter):\n",
    "            k += 1\n",
    "        \n",
    "            # Construct and solve quadratic model  \n",
    "            def m_k(x_k, p): #initial_radius=initial_radius\n",
    "                # if norm(p) <= initial_radius:\n",
    "                return self.f(x_k) + self.df(x_k).T @ p + 0.5 * p.T @ self.d2f(x_k) @ p\n",
    "\n",
    "            # Evaluate actual to predicted reduction ratio\n",
    "            # Bk = d2f(x_k) for trust-region Newton method\n",
    "            acc_reduction = self.f(x_k) - self.f(x_k + p)\n",
    "            pred_reduction = m_k(x_k, np.zeros(np.shape(p))) - m_k(x_k, p=np.array(p))\n",
    "            p_k = acc_reduction/pred_reduction \n",
    "            # Note that since the step pk is obtained by minimizing the model mk over a region \n",
    "            # that includes the step p = 0, the predicted reduction will always be nonnegative.\n",
    "            # Thus if ρk is negative, the new objective value f (xk + pk) is greater than the \n",
    "            # current value f (xk), so the step must be rejected.\n",
    "            # if ρk is close to 1, there is good agreement between the model mk and the function \n",
    "            # over this step, so it is safe to expand the trust region for the next iteration.\n",
    "\n",
    "\n",
    "            # Record iteration information\n",
    "            info['rhos'].append(p_k)\n",
    "            info['Deltas'].append(current_radius)\n",
    "\n",
    "            # Adjust the size of the trustregion\n",
    "            if p_k < 0.25:\n",
    "                current_radius = 0.25 * current_radius\n",
    "            elif p_k > 0.75 and norm(p) == current_radius: # or abs(p.T @ p - Delta_k**2) < 1e-16: # norm(p) == current_radius is a safeguard against increasing Delta_k indefinitely, norm(p)=abs(p.T * p - Delta_k**2)\n",
    "                current_radius = min(2*current_radius, trust_radius_Delta)\n",
    "            else:\n",
    "                current_radius = current_radius \n",
    "            \n",
    "            # Accept step\n",
    "            if p_k > eta:\n",
    "                x_k_1 = x_k + p_k\n",
    "            else:\n",
    "                x_k_1 = x_k\n",
    "                # nTaken = nTaken + 1;\n",
    "                # info.xs(:,nTaken+1) = x_k;\n",
    "                # info.xind(nTaken+1) = k;\n",
    "\n",
    "\n",
    "            # Evaluate stopping condition: \n",
    "            if stopType == 'step':\n",
    "                # relative step length\n",
    "                stopCond = (norm(x_k - x_k_1)/norm(x_k_1) < tol)\n",
    "            elif stopType == 'grad':\n",
    "                # gradient norm\n",
    "                #stopCond = (norm(F.df(x_k)) < tol);\n",
    "                stopCond = (norm(self.df(x_k), ord=np.inf) < tol*(1 + abs(self.f(x_k))))\n",
    "            elif current_radius < 1e-6*trust_radius_Delta:\n",
    "                # Stop iteration if Delta_k shrank below 1e-6*Delta. Otherwise, if the model \n",
    "                # does not improve inspite of shinking, the algorithm would shrink Delta_k indefinitely.\n",
    "                print('Region of interest is to small. Terminating iteration.')\n",
    "\n",
    "            f_k = self.f(x_k)\n",
    "            info['stopCond'].append(stopCond)\n",
    "            info['xs'].append(x_k)\n",
    "            info['xind'][k] = nTaken\n",
    "            # info['xs'][:,nTaken+2:] = []\n",
    "            # info['xind'][nTaken+2:] = []\n",
    "            # info['rhos'][k+1:] = []  \n",
    "            # info['Deltas'][k+1:] = [] \n",
    "\n",
    "        return x_k, f_k, k, info \n",
    "\n",
    "\n",
    "    def flecther_reeves(self, x0, alpha0, alpha_max, tol, maxIter,\n",
    "                        ls ='linesearch', stop_type = 'grad', opts = {'c1': 1e-4, 'c2': 0.1}):\n",
    "\n",
    "        # Initialization\n",
    "        nIter = 0\n",
    "        x_k = x0\n",
    "        p_k = -self.df(x_k)\n",
    "        info = {'xs': [np.array(x_k)], 'alphas': [alpha0], 'betas': [], 'grads': [self.df(x_k)]}\n",
    "        stopCond = False\n",
    "\n",
    "        # Loop until convergence or maximum number of iterations\n",
    "        while not stopCond and nIter <= maxIter:\n",
    "\n",
    "            # Call line search to compute step length alpha_k\n",
    "            if ls.lower() =='linesearch':\n",
    "                alpha_k, _ = Functions.lineSearch(self, x_k=x_k, p_k=p_k, alpha0=alpha0,\n",
    "                            alpha_max=alpha_max, opts = opts) \n",
    "                \n",
    "            elif ls.lower() == 'backtracking':\n",
    "                alpha_k, _ = Functions.backtracking(self, x_k, p_k, alpha0, opts={'c1': 1e-4, 'rho': 0.9})\n",
    "                \n",
    "            # Update x\n",
    "            x_k_1 = x_k + alpha_k * p_k\n",
    "\n",
    "            # Update gradient  \n",
    "            grad = self.df(x_k_1)\n",
    "            info['grads'].append(grad)\n",
    "\n",
    "            # FR Method\n",
    "            beta = np.max((info['grads'][nIter+1].T * info['grads'][nIter+1]) / (info['grads'][nIter].T * info['grads'][nIter]), 0)\n",
    "\n",
    "            # # Compute new direction p1\n",
    "            p_k = -grad + beta * p_k\n",
    "\n",
    "            info['betas'].append(beta)\n",
    "            info['xs'].append(x_k_1)\n",
    "            info['alphas'].append(alpha_k)\n",
    "\n",
    "            # Stopping conditions\n",
    "            if stop_type.lower() == 'step':\n",
    "                # Compute relative step length\n",
    "                normStep = norm(x_k - x_k_1) / norm(x_k_1)\n",
    "                stopCond = (normStep < tol)\n",
    "            if stop_type.lower() == 'grad':\n",
    "                stopCond = (norm(self.df(x_k), np.inf)) < tol * (1 + abs(self.f(x_k)))\n",
    "\n",
    "            x_k = x_k_1\n",
    "            nIter += 1\n",
    "\n",
    "        # Assign output values\n",
    "        xMin = x_k\n",
    "        fMin = self.f(x_k)\n",
    "\n",
    "        return xMin, fMin, nIter, info\n",
    "    \n",
    "\n",
    "    def polak_ribiereCGmethod(self, x0, alpha0, tol, maxIter, alpha_max,\n",
    "                        ls ='linesearch', stop_type = 'grad'):\n",
    "\n",
    "        # Initialization\n",
    "        nIter = 0\n",
    "        x_k = x0\n",
    "        p_k = -self.df(x_k)\n",
    "        info = {'xs': [np.array(x_k)], 'alphas': [alpha0], 'betas': [], 'grads': [self.df(x_k)]}\n",
    "        stopCond = False\n",
    "\n",
    "        # Loop until convergence or maximum number of iterations\n",
    "        while not stopCond and nIter <= maxIter:\n",
    "\n",
    "            # Call line search to compute step length alpha_k\n",
    "            # print(x_k)\n",
    "            if ls.lower() =='linesearch':\n",
    "                alpha_k, _ = Functions.lineSearch(self, x_k=x_k, p_k=p_k, alpha0=alpha0,\n",
    "                            alpha_max=alpha_max, opts = {'c1': 1e-4, 'c2': 0.1}) \n",
    "                \n",
    "            elif ls.lower() == 'backtracking':\n",
    "                alpha_k, _ = Functions.backtracking(self, x_k, p_k, alpha0, opts={'c1': 1e-4, 'rho': 0.2})\n",
    "                \n",
    "            # Update x\n",
    "            x_k_1 = x_k + alpha_k * p_k\n",
    "\n",
    "            # Update gradient  \n",
    "            grad = self.df(x_k_1)\n",
    "            info['grads'].append(grad)\n",
    "\n",
    "            # Polak Method\n",
    "            beta = (info['grads'][nIter+1].T * (info['grads'][nIter+1] - info['grads'][nIter])) / norm(info['grads'][nIter])**2\n",
    "\n",
    "            # # Compute new direction p1\n",
    "            p_k = -grad + beta * p_k\n",
    "\n",
    "            info['betas'].append(beta)\n",
    "            info['xs'].append(x_k_1)\n",
    "            info['alphas'].append(alpha_k)\n",
    "\n",
    "            # Stopping conditions\n",
    "            if stop_type.lower() == 'step':\n",
    "                # Compute relative step length\n",
    "                normStep = norm(x_k - x_k_1) / norm(x_k_1)\n",
    "                stopCond = (normStep < tol)\n",
    "            if stop_type.lower() == 'grad':\n",
    "                stopCond = (norm(self.df(x_k), np.inf)) < tol * (1 + abs(self.f(x_k)))\n",
    "\n",
    "            x_k = x_k_1\n",
    "            nIter += 1\n",
    "\n",
    "        # Assign output values\n",
    "        xMin = x_k\n",
    "        fMin = self.f(x_k)\n",
    "\n",
    "        return xMin, fMin, nIter, info\n",
    "    \n",
    "\n",
    "    def BFGS_alg(self, x0, alpha0, tol, maxIter, stop_type = 'grad', \n",
    "                 ls='linesearch', alpha_max=None):\n",
    "\n",
    "        # Initialization\n",
    "        nIter = 0\n",
    "        x_k = x0\n",
    "        info = {'xs': [x0], 'alphas': [alpha0]}\n",
    "        stopCond = False\n",
    "        H0 = self.d2f(x_k) # np.eye(2)\n",
    "        rho_k = 0.9\n",
    "            \n",
    "        # Loop until convergence or maximum number of iterations\n",
    "        while not stopCond and nIter <= maxIter and norm(self.df(x_k)) > tol:\n",
    "\n",
    "            p_k = -H0.dot(self.df(x_k))\n",
    "            \n",
    "            # Call line search to compute step length alpha_k\n",
    "            if ls.lower() =='linesearch':\n",
    "                alpha_k, _ = Functions.lineSearch(self, x_k=x_k, p_k=p_k, alpha0=alpha0, \n",
    "                            alpha_max=alpha_max, opts = {'c1': 1e-4, 'c2': 0.99}) \n",
    "            else:\n",
    "                print('backtracking takes too long')\n",
    "\n",
    "            # Update x_k\n",
    "            x_k_1 = x_k + alpha_k * p_k\n",
    "\n",
    "            # Compute step diff s_k\n",
    "            s_k = x_k - x_k_1\n",
    "            \n",
    "            # must satify curvature condition s_k.T * y_k > 0\n",
    "            \n",
    "            # Compute the gradient difference g_k\n",
    "            y_k = self.df(x_k) - self.df(x_k_1)\n",
    "\n",
    "            # Compute rho\n",
    "            rho_k = 1/(y_k.T @ s_k)\n",
    "\n",
    "            # Compute the new inverse hessian\n",
    "            H1 = (np.eye(2) - rho_k * s_k @ y_k.T) @ H0 @ (np.eye(2) - rho_k * y_k @ s_k.T) + rho_k * (s_k @ s_k.T)\n",
    "            \n",
    "            H0 = H1\n",
    "            x_k = x_k_1\n",
    "            nIter += 1\n",
    "            \n",
    "            # Store iteration info\n",
    "            info['xs'].append(x_k)\n",
    "            info['alphas'].append(alpha_k)\n",
    "\n",
    "            # Stopping conditions\n",
    "            if stop_type.lower() == 'step':\n",
    "                # Compute relative step length\n",
    "                normStep = norm(x_k - x_k_1) / norm(x_k_1)\n",
    "                stopCond = (normStep < tol)\n",
    "            elif stop_type.lower() == 'grad':\n",
    "                stopCond = (norm(self.df(x_k),  np.inf) < tol * (1 + abs(self.f(x_k)))) # np.inf\n",
    "\n",
    "\n",
    "        # Assign output values\n",
    "        xMin = x_k\n",
    "        fMin = self.f(x_k)\n",
    "\n",
    "        return xMin, fMin, nIter, info\n",
    "\n",
    "\n",
    "    def convergence_history(self, info, xMin, x0, p, H=None, plot_type=None):\n",
    "        \n",
    "        # convert info['xs'] to numpy array\n",
    "        arr = np.zeros(shape=(2,len(info['xs'])))\n",
    "        for i in range(len(info['xs'])):\n",
    "            x = info['xs'][i][0]\n",
    "            y = info['xs'][i][1]\n",
    "            arr[0][i] = x\n",
    "            arr[1][i] = y\n",
    "        \n",
    "        if xMin is None:\n",
    "            xMin = (arr[0][-1], arr[1][-1])\n",
    "        \n",
    "        shape = arr.shape[1]\n",
    "        if p == 'M':\n",
    "            p = 2\n",
    "            if H is not None:\n",
    "                M = H\n",
    "            else:\n",
    "                M = self.d2f(xMin)  # M is the Hessian at the solution, M has to be symmetric positive definite\n",
    "            \n",
    "            # Convergence of iterates: || x_k - xMin ||_M\n",
    "            err = info['xs'] - np.tile(xMin, (2, shape))            \n",
    "            # err = xs - np.tile(x_min[:, np.newaxis], (1, xs.shape[1]))\n",
    "            con_x = [np.sqrt(np.dot(err[k].T, M.dot(err[k]))) for k in range(shape)]\n",
    "        else:\n",
    "            # Convergence of iterates: || x_k - xMin ||_p\n",
    "            # print((arr - np.array([xMin[0], xMin[1]]*shape).reshape(2,shape)).shape)\n",
    "            con_x = np.sum(np.abs(arr - np.array([xMin[0], xMin[1]]*shape).reshape(2,shape))**p, axis=0)**(1/p)\n",
    "\n",
    "        if self.f is not None:\n",
    "            # Convergence of function values: f(x_k) - f(xMin)\n",
    "            con_f = [self.f([arr[0,k], arr[1,k]]) - self.f(xMin) for k in range(shape)]\n",
    "\n",
    "            # Convergence of gradient: || f(x_k)||_p\n",
    "            con_df = [np.sum(np.abs(self.df([arr[0,k], arr[1,k]]))**p)**(1/p) for k in range(shape)]\n",
    "        else:\n",
    "            con_f = []\n",
    "            con_df = []\n",
    "\n",
    "\n",
    "        conInfo = {'x': con_x, 'f': con_f, 'df': con_df} # con\n",
    "\n",
    "        if plot_type == 'Q-convergence':\n",
    "\n",
    "            # Q convergence \n",
    "            p = 1\n",
    "            rs1 = conInfo['x'][1:] / (conInfo['x'][:-1]**p)\n",
    "            ps = 1.4\n",
    "            rss = conInfo['x'][1:] / (conInfo['x'][:-1]**ps)\n",
    "            p = 2\n",
    "            rs2 = conInfo['x'][1:] / (conInfo['x'][:-1]**p)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(rs1)\n",
    "            plt.title('Q convergence')\n",
    "            plt.legend(['p=1'])\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(rs1)\n",
    "            plt.plot(rss, 'k--')\n",
    "            plt.title('Q convergence')\n",
    "            plt.legend(['p=1', 'p='+str(ps)])\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(rs1)\n",
    "            plt.plot(rss, 'k--')\n",
    "            plt.plot(rs2, 'r-.')\n",
    "            plt.xlabel('k')\n",
    "            plt.ylabel(r'$\\frac{||x_{k+1} - x^* ||}{||x_k - x^*||^p}$')\n",
    "            plt.title('Q convergence')\n",
    "            plt.legend(['p=1', 'p='+str(ps), 'p=2'])\n",
    "            plt.grid(True)\n",
    "\n",
    "        elif plot_type == 'Algebraic_convergence':\n",
    "            # Algebraic convergence\n",
    "            plt.figure()\n",
    "            plt.loglog(conInfo['x'])\n",
    "            plt.title('Algebraic convergence')\n",
    "            plt.xlabel('ln(k)')\n",
    "            plt.ylabel('ln(||x_k - x*||)')\n",
    "            plt.grid(True)  \n",
    "        \n",
    "        elif plot_type == 'Exp_convergence':\n",
    "            # Exponential convergence\n",
    "            plt.figure()\n",
    "            plt.semilogy(conInfo['x'])\n",
    "            plt.title('Exponential convergence')\n",
    "            plt.xlabel('k')\n",
    "            plt.ylabel('ln(||x_k - x*||)')\n",
    "            plt.xlim(0, len(conInfo['x'])-1)\n",
    "            plt.grid(True)\n",
    "\n",
    "        elif plot_type == 'Grad_over_iters':\n",
    "            # Gradient norm over iterates\n",
    "            plt.figure()\n",
    "            plt.semilogy(conInfo['df'])\n",
    "            plt.title('Exponential convergence: ||grad(f)||')\n",
    "            plt.xlabel('k')\n",
    "            plt.ylabel('ln||grad(x_k)||')\n",
    "            plt.grid(True)\n",
    "\n",
    "        elif plot_type == 'Function_values':\n",
    "            # Gradient norm convergence to a stationary point\n",
    "            sdfConSteep = [sorted(conInfo['df'], reverse=True)]\n",
    "            min_dfj = np.zeros(len(sdfConSteep))\n",
    "\n",
    "            plt.figure()\n",
    "            for j in range(len(sdfConSteep)):\n",
    "                min_dfj[j] = sdfConSteep[j][j]\n",
    "            plt.loglog(range(1, len(sdfConSteep)+1), min_dfj)\n",
    "\n",
    "\n",
    "            L = np.max(np.abs(np.linalg.eig(self.d2f(x0)))) # Lipschitz constant \n",
    "            plt.loglog(range(2, len(sdfConSteep)+1, 2), np.sqrt(2 * 2 * (self.f(x0) - self.f(info['xs'][-1])) / range(2, len(sdfConSteep)+1, 2)), 'rx-')\n",
    "            plt.loglog(range(2, len(sdfConSteep)+1, 2), 4 * 2 * np.linalg.norm(x0 - info['xs'][-1]) / range(2, len(sdfConSteep)+1, 2), 'ko-')\n",
    "            plt.grid(True)\n",
    "            plt.title(r\"Approximate convergence $\\min_{1,...,k} \\|\\nabla (f (x_k) )\\|$\",\n",
    "                    fontdict={'verticalalignment': 'bottom'}, fontsize=12, position=(0.5, 0.93))\n",
    "            plt.xlabel('ln k')\n",
    "            plt.ylabel(r'ln $\\min_{1,...,k} \\| \\nabla(f(x_{k})) \\|$', fontsize=12)\n",
    "        \n",
    "        elif plot_type == None:\n",
    "            return {'x': con_x, 'f': con_f, 'df': con_df}\n",
    "\n",
    "    \n",
    "    def approximate_grad_convergence(self, frame1, coord1, coord2=None, num_functions=2, frame2=None, title_plot='Dog leg method'):\n",
    "        if num_functions == 2:\n",
    "            ys_1 = []\n",
    "            for i in range(len(frame1)):\n",
    "                ys_1.append(norm(self.df(frame1[i])))\n",
    "            x_1 = np.arange(1,len(frame1)+1)\n",
    "\n",
    "            ys_2 = []\n",
    "            for i in range(len(frame2)):\n",
    "                ys_2.append(norm(self.df(frame2[i])))\n",
    "            x_2 = np.arange(1,len(frame2)+1)\n",
    "\n",
    "            plt.figure()\n",
    "            plt.loglog(x_1, ys_1, label=f'Coordinate {coord1}')\n",
    "            plt.loglog(x_2, ys_2, label=f'Coordinate {coord2}')\n",
    "            plt.title(f'Gradient Norm Convergence rate of {title_plot}')\n",
    "            plt.xlabel(r'$\\ln k$')\n",
    "            plt.ylabel(r'$\\ln||\\nabla f(x,y)$||')\n",
    "            plt.xlim(1,max(x_1[-1],x_2[-1]))\n",
    "            plt.legend()\n",
    "\n",
    "        elif num_functions == 1:\n",
    "            ys = []\n",
    "            for i in range(len(frame1)):\n",
    "                ys.append(norm(self.df(frame1[i])))\n",
    "            x = np.arange(1,len(frame1)+1)\n",
    "\n",
    "            plt.figure()\n",
    "            plt.loglog(x, ys, label=f'Coordinate {coord1}')\n",
    "            plt.title('Gradient Norm Convergence rate of Newton Method')\n",
    "            plt.xlabel('Iterations')\n",
    "            plt.ylabel(r'||$\\nabla f(x,y)$||')\n",
    "            plt.xlim(1,x[-1])\n",
    "            plt.legend()\n",
    "\n",
    "\n",
    "    def file_formatting(self,file):\n",
    "\n",
    "        string_values = file[:, 1][0].strip('][')\n",
    "        numeric_values = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', string_values)\n",
    "        xs = np.array([float(value) for value in numeric_values])\n",
    "        xs = xs.reshape(-1, 2)\n",
    "\n",
    "        deltas = file[:, 0][0].strip('][')\n",
    "        deltas = deltas.split(',')\n",
    "        deltas = [float(i) for i in deltas]\n",
    "        deltas = np.array(deltas)\n",
    "\n",
    "        info = {'xs': [], 'deltas': [], 'xind': []}\n",
    "        for i in range(len(xs)):\n",
    "            info['xs'].append(xs[i])\n",
    "            info['deltas'].append(deltas[i])\n",
    "            info['xind'].append(i)\n",
    "\n",
    "        return info\n",
    "\n",
    "\n",
    "    def visualize_convergence(self, info, X, Y, Z, mode, max_steps=None):\n",
    "        TIME_PAUSE = 0.5\n",
    "        if max_steps is None:\n",
    "            MAX_STEPS = len(info['xs'])\n",
    "        else:\n",
    "            MAX_STEPS = min(max_steps, len(info['xs']))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.contourf(X, Y, Z, 20)\n",
    "        bbox = plt.axis()\n",
    "\n",
    "        if mode == 'final':\n",
    "            plt.plot(info['xs'][:][0], info['xs'][:][1], '-or', linewidth=2, markersize=3)\n",
    "            plt.title('Convergence')\n",
    "        elif mode == 'iterative':\n",
    "            n_iter = np.shape(info['xs'])[1]\n",
    "\n",
    "            for j in range(MAX_STEPS):\n",
    "                plt.clf()\n",
    "                plt.contour(X, Y, Z, 20)\n",
    "                plt.plot(info['xs'][j + 1][0], info['xs'][j + 1][1], '-or', linewidth=2, markersize=3)\n",
    "                plt.plot(info['xs'][j][0], info['xs'][j][1], '-xr', linewidth=2, markersize=8)\n",
    "\n",
    "                if 'Deltas' in info and j > 0:\n",
    "                    plt.plot(info['xs'][j - 1][0] + np.cos(np.arange(0, 2 * np.pi, 0.05)) * info['Deltas'][info['xind'][j]],\n",
    "                            info['xs'][j - 1][1] + np.sin(np.arange(0, 2 * np.pi, 0.05)) * info['Deltas'][info['xind'][j]],\n",
    "                            ':k', linewidth=2)\n",
    "\n",
    "                plt.axis(bbox)\n",
    "                plt.title(f'Convergence: steps 1 : {j + 1}')\n",
    "                plt.pause(TIME_PAUSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
